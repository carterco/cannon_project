{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is meant as a walkthrough of the steps I followed to use The Cannon to derive individual \n",
    "# abundances for Gaia RVS spectra. Warning: Not a perfect \"plug and chug\" example (more of a hodepodge of\n",
    "# several different notebooks), so use with caution and please make improvements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e79a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import glob\n",
    "import gzip\n",
    "import csv\n",
    "from ast import literal_eval\n",
    "import os\n",
    "from IPython.utils import io\n",
    "\n",
    "from TheCannon import apogee \n",
    "from TheCannon import dataset\n",
    "from TheCannon import model\n",
    "import astropy.io.fits as pyfits\n",
    "import astropy.table as tbl\n",
    "\n",
    "# Useful tool to look at RVS spectra quickly, but non-essential\n",
    "from gaia_tools.load.spec import read_spec_internal\n",
    "# Change this to where your Gaia data lives\n",
    "os.environ['GAIA_TOOLS_DATA']='gaia_data/'\n",
    "os.path.basename\n",
    "\n",
    "# Plotting settings \n",
    "import matplotlib.pyplot as plt;\n",
    "import matplotlib as mpl;\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "import matplotlib.cm as cm;\n",
    "import matplotlib.colors as colors\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "cmap = mpl.rcParams['xtick.labelsize'] = 12;mpl.rcParams['ytick.labelsize'] = 12;mpl.rcParams['font.weight'] = 'medium';mpl.rcParams['axes.linewidth'] = 1.5;mpl.rcParams['xtick.major.width'] = 1.5;mpl.rcParams['xtick.minor.width'] = 0.75;mpl.rcParams['xtick.minor.visible'] = True;mpl.rcParams['ytick.major.width'] = 1.5;mpl.rcParams['ytick.minor.width'] = 0.75;mpl.rcParams['ytick.minor.visible'] = True\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ca5ea",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daced76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function for colorbars\n",
    "# Source: https://joseph-long.com/writing/colorbars/\n",
    "def colorbar(mappable):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    import matplotlib.pyplot as plt\n",
    "    last_axes = plt.gca()\n",
    "    ax = mappable.axes\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"10%\", pad=0.1)\n",
    "    cbar = fig.colorbar(mappable, cax=cax)\n",
    "    plt.sca(last_axes)\n",
    "    # plt.subplots_adjust(wspace=.3)\n",
    "    return cbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d924a3",
   "metadata": {},
   "source": [
    "## Cross-matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e10d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the SNR of RVS spectra\n",
    "# Source: Somewhere in The Cannon...\n",
    "def SNR(flux, uncertainty):\n",
    "    SNR = np.zeros(len(flux))\n",
    "    for i in range(len(flux)):\n",
    "        flux_ = np.fromstring(flux[i].strip(\"[]\"), sep=',')\n",
    "        flux_error_ = np.fromstring(uncertainty[i].strip(\"[]\"), sep=',')\n",
    "        SNR_ = np.nanmean(flux_)/np.nanmean(flux_error_)\n",
    "        SNR[i] = SNR_\n",
    "    return SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RVS data from folder\n",
    "gaia_rvs_files=glob.glob('gaia_data/*.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get APOGEE data from .fits file\n",
    "fits_file='apogee_data/allStar-dr17-synspec_rev1.fits'\n",
    "dfapogee = Table.read(fits_file,format='fits')\n",
    "names = [name for name in dfapogee.colnames if len(dfapogee[name].shape) <= 1]\n",
    "dfapogee = dfapogee[names].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small merge test case using (1) RVS file\n",
    "for path in gaia_rvs_files[:1]:\n",
    "    print('Processing', path, '...')\n",
    "    dfgaia = pd.read_csv(path, skiprows=84, names=['source_id','solution_id','ra','dec','flux','flux_error','combined_transits','combined_ccds','deblended_ccds'])\n",
    "\n",
    "df_merged=pd.merge(dfgaia, dfapogee, \n",
    "         left_on='source_id',\n",
    "        right_on='GAIAEDR3_SOURCE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5662bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, gives us the column names for the next step\n",
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80789a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new empty dataframe with the columns from the test case\n",
    "# Can do this in a more elegant way...\n",
    "new_df_merged = pd.DataFrame(columns=df_merged.columns)\n",
    "\n",
    "# Iterate through all the RVS files and append to the new dataframe -- will take a bit!\n",
    "for path in gaia_rvs_files:\n",
    "    dfgaia = pd.read_csv(path, skiprows=84, names=['source_id','solution_id','ra','dec','flux','flux_error','combined_transits','combined_ccds','deblended_ccds'])\n",
    "    \n",
    "    df_merged=pd.merge(dfgaia, dfapogee, \n",
    "         left_on='source_id',\n",
    "        right_on='GAIAEDR3_SOURCE_ID')\n",
    "    \n",
    "    new_df_merged = pd.concat([df_merged, new_df_merged], ignore_index=True)\n",
    "\n",
    "# Save the new dataframe so we don't have to do that again!\n",
    "new_df_merged.to_csv('crossmatched_stars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeebcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can read in the new dataframe\n",
    "df2 = pd.read_csv('crossmatched_stars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we don't have SNR in the Gaia data, so let's add that\n",
    "df2_SNR = SNR(df2['flux'], df2['flux_error'])\n",
    "other = pd.DataFrame({'gaia_SNR': df2_SNR}) # Be better and don't name things this way...\n",
    "new = df2.join(other)\n",
    "\n",
    "# Save a new dataframe with Gaia SNR\n",
    "new.to_csv('crossmatched_stars_v3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745e19a",
   "metadata": {},
   "source": [
    "## Define reference set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 3D array (arr) by num\n",
    "def sample(arr, num):\n",
    "    Arr = arr\n",
    "    new_3D_Arr = []\n",
    "    for row in arr:\n",
    "        new_row = row[::num]\n",
    "        new_3D_Arr.append(new_row)\n",
    "    return new_3D_Arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f83cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our clean data set\n",
    "starbad = 2**23 # \"BAD overall for star: set if any of TEFF, LOGG, CHI2, COLORTE, ROTATION, SN error are set, or any parameter is near grid edge (GRIDEDGE_BAD is set in any PARAMFLAG)\"\n",
    "apo_clean = (np.bitwise_and(df2['ASPCAPFLAG'], starbad) == 0) & (df2['SNR'] > 200) & (df2['gaia_SNR'] > 100) & (df2['TEFF'] > 3500) & (df2['TEFF'] < 5500) & (df2['LOGG'] < 3.6)\n",
    "indices = np.where(apo_clean)[0] # May need to tweak this if your data looks different\n",
    "df2_clean = df2['source_id'][indices].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db187a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our reference data set by even sampling\n",
    "# Let's look at only (3) dimensions first\n",
    "LOGG_ = list(df2['LOGG'][indices])\n",
    "TEFF_ = list(df2['TEFF'][indices])\n",
    "FEH_ = list(df2['FE_H'][indices])\n",
    "\n",
    "arr_3D = np.array([LOGG_, TEFF_, FEH_])\n",
    "\n",
    "num = 10\n",
    "test_sample = sample(arr_3D, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this looks how you would expect...\n",
    "new_LOGG = test_sample[0]\n",
    "new_TEFF = test_sample[1]\n",
    "new_FEH = test_sample[2]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,4.6))\n",
    "axs[0].scatter(TEFF_, LOGG_, c=FEH_, s=5, cmap='coolwarm')\n",
    "im = axs[1].scatter(new_TEFF, new_LOGG, c=new_FEH, s=5, cmap='coolwarm')\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    axs[i].set_xlabel('T$_{eff}$', size=12)\n",
    "    axs[i].set_ylabel('logg', size=12)\n",
    "    axs[i].set_xlim(5600, 3400)\n",
    "    axs[i].grid()\n",
    "\n",
    "axs[0].set_title('Before Sampling', size=12)\n",
    "axs[0].text(4200,3.4,\"12,305 stars\", size=12)\n",
    "axs[1].set_title('After Sampling', size=12)\n",
    "axs[1].text(4200,3.4,\"1,231 stars\", size=12)\n",
    "\n",
    "cbar = colorbar(im)\n",
    "cbar.set_label('[Fe/H]', size=12)\n",
    "plt.tight_layout(h_pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this seems reasonable, then you can proceed with this\n",
    "new_idx = indices[::num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels we want evenly sampled (all)\n",
    "f_LOGG = df2['LOGG'][new_idx]\n",
    "f_TEFF = df2['TEFF'][new_idx]\n",
    "f_FE_H = df2['FE_H'][new_idx]\n",
    "f_MG_FE = df2['MG_FE'][new_idx]\n",
    "f_SI_FE = df2['SI_FE'][new_idx]\n",
    "f_CA_FE = df2['CA_FE'][new_idx]\n",
    "f_NI_FE = df2['NI_FE'][new_idx]\n",
    "f_APOGEE_ID = df2['APOGEE_ID'][new_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20866966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put them into a nice dataframe\n",
    "ref_labels = pd.DataFrame({'APOGEE_ID': f_APOGEE_ID, 'Teff': f_TEFF, 'logg': f_LOGG, '[Fe/H]': f_FE_H, '[Mg/Fe]': f_MG_FE, '[Si/Fe]': f_SI_FE, '[Ca/Fe]': f_CA_FE, '[Ni/Fe]': f_NI_FE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save these! This will be our reference set\n",
    "ref_labels.to_csv('ref_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae6cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need the reference IDs, but let's fix that they are strings!\n",
    "fix = df2['APOGEE_ID'][new_idx].astype(\"string\")\n",
    "fix_ids = fix.str[2:-1]\n",
    "fix_ids.to_csv('ref_ids.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41f542",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792904d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful defintions (modified)\n",
    "\n",
    "# Modified for this particular case\n",
    "def load_rvs_spec(source_ids, assume_unique=False):\n",
    "    \"\"\"\n",
    "    NAME:\n",
    "        load_rvs_spec\n",
    "    PURPOSE:\n",
    "        Read corresponding RVS spectra for a list of source id\n",
    "    INPUT:\n",
    "        source_ids (int, list, ndarray): source id\n",
    "        assume_unique (bool): whether to assume the list of source id is unique\n",
    "    OUTPUT:\n",
    "        wavelength grid, RVS spectra flux row matched to source_id, RVS spectra corresponding flux uncertainty\n",
    "    HISTORY:\n",
    "        2022-06-16 - Written - Henry Leung (UofT)\n",
    "    \"\"\"\n",
    "    base_path = 'gaia_data/'\n",
    "    wavelength_grid = np.arange(846, 870.01, 0.01)\n",
    "    return read_spec_internal(\n",
    "        source_ids=source_ids,\n",
    "        assume_unique=assume_unique,\n",
    "        base_path=base_path,\n",
    "        wavelength_grid=wavelength_grid,\n",
    "    )\n",
    "\n",
    "# Modified for this particular case\n",
    "def load_spectra(data_dir):\n",
    "    \"\"\" Reads wavelength, flux, and flux uncertainty data from apogee fits files\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir: str\n",
    "        Name of the directory containing all of the data files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    wl: ndarray\n",
    "        Rest-frame wavelength vector\n",
    "\n",
    "    fluxes: ndarray\n",
    "        Flux data values\n",
    "\n",
    "    ivars: ndarray\n",
    "        Inverse variance values corresponding to flux values\n",
    "    \"\"\"\n",
    "    print(\"Loading spectra from directory %s\" %data_dir)\n",
    "    files = list(sorted([data_dir + \"/\" + filename\n",
    "             for filename in os.listdir(data_dir) if filename.endswith('fits')]))\n",
    "    nstars = len(files)  \n",
    "    for jj, fits_file in enumerate(files):\n",
    "        file_in = pyfits.open(fits_file)\n",
    "        flux = np.array(file_in[1].data)\n",
    "        if jj == 0:\n",
    "            npixels = len(flux)\n",
    "            fluxes = np.zeros((nstars, npixels), dtype=float)\n",
    "            ivars = np.zeros(fluxes.shape, dtype=float)\n",
    "            start_wl = file_in[1].header['CRVAL1']\n",
    "            diff_wl = file_in[1].header['CDELT1']\n",
    "            val = diff_wl * (npixels) + start_wl\n",
    "            wl_full_log = np.arange(start_wl,val, diff_wl)\n",
    "            wl_full = [10 ** aval for aval in wl_full_log]\n",
    "            wl = np.array(wl_full)\n",
    "        flux_err = np.array((file_in[2].data))\n",
    "        badpix = apogee.get_pixmask(flux, flux_err)\n",
    "        ivar = np.zeros(npixels)\n",
    "        ivar[~badpix] = 1. / flux_err[~badpix]**2\n",
    "        fluxes[jj,:] = flux\n",
    "        ivars[jj,:] = ivar\n",
    "    # Convert filenames to actual IDs\n",
    "    names = np.array([f.split('dr17-')[1].split('.fits')[0] for f in files])\n",
    "    print(\"Spectra loaded\")\n",
    "    # Make sure they are numpy arrays\n",
    "    return np.array(names), np.array(wl), np.array(fluxes), np.array(ivars) \n",
    "\n",
    "\n",
    "# Modified to include the relevant label names for this case\n",
    "def load_labels(filename):\n",
    "    \"\"\" Extracts reference labels from a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        Name of the file containing the table of reference labels\n",
    "\n",
    "    ids: array\n",
    "        The IDs of stars to retrieve labels for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels: ndarray\n",
    "        Reference label values for all reference objects\n",
    "    \"\"\"\n",
    "    print(\"Loading reference labels from file %s\" %filename)\n",
    "    data = pd.read_csv(filename)\n",
    "    ids = data['ID']\n",
    "    inds = ids.argsort()\n",
    "    ids = ids[inds]\n",
    "    teff = data['Teff'] \n",
    "    teff = teff[inds]\n",
    "    logg = data['logg']\n",
    "    logg = logg[inds]\n",
    "    mh = data['[Fe/H]']\n",
    "    mh = mh[inds]\n",
    "    mgfe = data['[Mg/Fe]']\n",
    "    mgfe = mgfe[inds]\n",
    "    sife = data['[Si/Fe]']\n",
    "    sife = sife[inds]\n",
    "    cafe = data['[Ca/Fe]']\n",
    "    cafe = cafe[inds]\n",
    "    nife = data['[Ni/Fe]']\n",
    "    nife = nife[inds]\n",
    "    return np.vstack((teff,logg,mh,mgfe,sife,cafe,nife)).T \n",
    "\n",
    "# Find inverse variance values corresponding to flux values\n",
    "# Source: https://github.com/annayqho/TheCannon/blob/8010a0a5dc9a3f9bb91efa79d7756f79b3c7ba9a/TheCannon/apogee.py\n",
    "def find_ivar(flux, flux_err):\n",
    "    npixels = len(flux)\n",
    "    badpix = apogee.get_pixmask(flux, flux_err)\n",
    "    ivar = np.zeros(npixels)\n",
    "    ivar[~badpix] = 1. / flux_err[~badpix]**2\n",
    "    return ivar\n",
    "\n",
    "# Convert string fluxes and errors to integers\n",
    "def pre_processing(df):\n",
    "    df[['flux', 'flux_error']] = df[['flux', 'flux_error']].astype('string')\n",
    "    df['source_id'] = df['source_id'].astype('int64')\n",
    "    return df\n",
    "\n",
    "# This automates The Cannon steps\n",
    "def auto_cannon(chunked_files, count):    \n",
    "    # Smaller dataframe\n",
    "    dfgaia = pd.DataFrame(columns={'source_id', 'flux', 'flux_error'})\n",
    "    dfgaia = pre_processing(dfgaia)\n",
    "\n",
    "    for path in chunked_files:\n",
    "        temp_gaia = pd.read_csv(path, skiprows=84, names=['source_id','solution_id','ra','dec','flux','flux_error','combined_transits','combined_ccds','deblended_ccds'], \n",
    "                                usecols = ['source_id','flux','flux_error'], dtype = {'source_id':'int64', 'flux':'string','flux_error':'string'}, low_memory = True)\n",
    "        dfgaia = pd.concat([temp_gaia, dfgaia], ignore_index=True)   \n",
    "    \n",
    "    arr_flux = []\n",
    "    arr_flux_error = []\n",
    "    \n",
    "    # Need to convert strings to arrays    \n",
    "    for i in range(len(dfgaia['flux'])): \n",
    "        arr_flux.append(np.fromstring(dfgaia['flux'][i].strip(\"[]\"), sep=',')) \n",
    "        arr_flux_error.append(np.fromstring(dfgaia['flux_error'][i].strip(\"[]\"), sep=','))\n",
    "\n",
    "    # Define test set\n",
    "    test_flux = np.nan_to_num(np.array(arr_flux))\n",
    "    test_errs = np.nan_to_num(np.array(arr_flux_error), nan=999)\n",
    "    test_ID = dfgaia['source_id']\n",
    "    test_ivar = []\n",
    "\n",
    "    for i in range(len(test_flux)):\n",
    "        ivar = find_ivar(test_flux[i], test_errs[i])\n",
    "        test_ivar.append(ivar)\n",
    "\n",
    "    test_ivar = np.array(test_ivar)\n",
    "    \n",
    "    ds = dataset.Dataset(wl, tr_ID, tr_flux, tr_ivar, tr_label, test_ID, test_flux, test_ivar)\n",
    "    ds.set_label_names(['T_{eff}', '\\log g', '[Fe/H]', '[Mg/Fe]', '[Si/Fe]', '[Ca/Fe]', '[Ni/Fe]'])\n",
    "    \n",
    "    md = model.CannonModel(2, useErrors=False)\n",
    "    md.fit(ds)\n",
    "    \n",
    "    label_errs = md.infer_labels(ds)\n",
    "    test_labels = ds.test_label_vals\n",
    "    \n",
    "    opt_err = label_errs[0]\n",
    "    chisqs = label_errs[1]\n",
    "    \n",
    "    snr = ds.test_SNR\n",
    "    \n",
    "    model_teff = [i[0] for i in test_labels]\n",
    "    model_logg = [i[1] for i in test_labels]\n",
    "    model_feh = [i[2] for i in test_labels]\n",
    "    model_mgfe = [i[3] for i in test_labels]\n",
    "    model_sife = [i[4] for i in test_labels]\n",
    "    model_cafe = [i[5] for i in test_labels]\n",
    "    model_nife = [i[6] for i in test_labels]\n",
    "    \n",
    "    err_teff = [i[0] for i in opt_err]\n",
    "    err_logg = [i[1] for i in opt_err]\n",
    "    err_feh = [i[2] for i in opt_err]\n",
    "    err_mgfe = [i[3] for i in opt_err]\n",
    "    err_sife = [i[4] for i in opt_err]\n",
    "    err_cafe = [i[5] for i in opt_err]\n",
    "    err_nife = [i[6] for i in opt_err]\n",
    "    \n",
    "    infer_spec = md.infer_spectra(ds)\n",
    "    model_spec = md.model_spectra\n",
    "    \n",
    "    \n",
    "    model_df = pd.DataFrame({'source_id': test_ID, 'model_teff': model_teff, \n",
    "                            'model_logg': model_logg, 'model_feh': model_feh, 'model_mgfe': model_mgfe,\n",
    "                            'model_sife': model_sife, 'model_cafe': model_cafe, 'model_nife': model_nife,\n",
    "                            'err_teff': err_teff, 'err_logg': err_logg, 'err_feh': err_feh, \n",
    "                            'err_mgfe': err_mgfe, 'err_sife': err_sife, 'err_cafe': err_cafe,\n",
    "                            'err_nife': err_nife, 'SNR': snr, 'chisqs': chisqs})\n",
    "    model_df.to_hdf('derived_parameters/gaiarvs_params_chunk_'+str(count)+'.h5', key='df')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set steps\n",
    "\n",
    "# Read-in crossmatched data\n",
    "df = pd.read_csv('crossmatched_stars_v3.csv')\n",
    "\n",
    "# We only need wavelength\n",
    "# Can do this in a more elegant way...\n",
    "wavelength, flux, flux_err = load_rvs_spec(6772307906968335104)\n",
    "\n",
    "# Read-in IDs for reference stars in crossmatched data\n",
    "df1 = pd.read_csv(\"/users/carterco/2nd_yr_project/ref_ids.csv\",header=None, names=['IDs']).astype(\"string\")\n",
    "\n",
    "# All IDs in crossmatched data\n",
    "ap_id = df['APOGEE_ID']\n",
    "ap_id = list(ap_id.str[2:-1])\n",
    "\n",
    "# Find reference stars in crossmatched data\n",
    "order = []\n",
    "\n",
    "for i, j in enumerate(df1['IDs']):\n",
    "    try:\n",
    "        order.append(ap_id.index(j))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Read in reference flux and errors\n",
    "df2 = pd.read_csv('gaia_fluxes.csv', header=None)\n",
    "df3 = pd.read_csv('gaia_flux_errs.csv', header=None)\n",
    "\n",
    "# Clean up reference fluxes and errors\n",
    "df2 = df2.fillna(0)\n",
    "df3 = df3.replace(np.nan, 999)\n",
    "\n",
    "# Make sure they are arrays\n",
    "fluxes = df2.to_numpy()\n",
    "flux_errs = df3.to_numpy()\n",
    "\n",
    "# Calculate ivars\n",
    "all_ivars = []\n",
    "\n",
    "for i in range(len(fluxes)):\n",
    "    ivar = find_ivar(fluxes[i], flux_errs[i])\n",
    "    all_ivars.append(ivar)\n",
    "    \n",
    "all_ivars = np.array(all_ivars)\n",
    "\n",
    "# All reference stars\n",
    "all_IDs = np.array(df['APOGEE_ID'].str[2:-1][order])\n",
    "all_labels = load_labels(\"ref_labels.csv\")\n",
    "wl = wavelength\n",
    "\n",
    "# All reference stars to training set\n",
    "tr_flux = fluxes\n",
    "tr_ID = all_IDs\n",
    "tr_label = all_labels\n",
    "tr_ivar = all_ivars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with gaia RVS files:\n",
    "gaia_rvs_files = glob.glob('gaia_data/*.gz')\n",
    "\n",
    "# Split data into digestible chunks\n",
    "chunks = []\n",
    "I = np.arange(0, 3300, 100)\n",
    "\n",
    "for i, j in enumerate(I):\n",
    "    try:\n",
    "        hold = gaia_rvs_files[I[i]:I[i+1]]\n",
    "        chunks.append(hold)\n",
    "    except:\n",
    "        hold = gaia_rvs_files[I[i]:len(gaia_rvs_files)-1]\n",
    "        chunks.append(hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b901c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run The Cannon for each data chunk and keep track of which \"chunk\"\n",
    "counter = 0\n",
    "for chunk in chunks:\n",
    "    counter+=1\n",
    "    try:\n",
    "        auto_cannon(chunk, counter)\n",
    "        print(\"Done with chunk \"+str(counter))\n",
    "    except:\n",
    "        print(\"Failed at chunk \"+str(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should end up with a folder of files with derived labels, which you can append as you'd like!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
